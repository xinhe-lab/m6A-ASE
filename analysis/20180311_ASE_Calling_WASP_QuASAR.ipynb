{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASE calling pipeline\n",
    "\n",
    "Mostly based on Yanyu's work in 2017. The pipeline aligns `fastq` sequences to `bam` using `STAR`, then adjust the mapping via `WASP` to account for allele specificity, and finally call genotype and ASE via `QuASAR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of procedure\n",
    "\n",
    "1. Gather require resources\n",
    "2. Align reads to genome\n",
    "3. Remove biased reads\n",
    "4. Genotype and ASE calling\n",
    "\n",
    "The pipeline is implemented in SoS as displayed in the rest of this SoS notebook. The pipeline can be executed from this notebook directly on local or remote computer (with caveats see below). For more information see [SoS website](https://github.com/vatlab/SOS).\n",
    "\n",
    "- Some software used (`STAR`, `samtools`, `bedtools`) are loaded from online `docker` images. This is me being lazy to avoid installing these software on a local computer (which should not take much time anyways). But some cluster system, such as UChicago RCC, does not support `docker`. Also here I did not configure cluster job queues with `task` option. So this notebook is intended to run on a local workstation as is, though involves minimal changes to get it work distributively on cluster, given required software are installed there.\n",
    "\n",
    "To run the pipeline:\n",
    "\n",
    "```\n",
    "sos run nb.ipynb hg19_reference\n",
    "sos run nb.ipynb obtain_samples\n",
    "sos run nb.ipynb default\n",
    "```\n",
    "\n",
    "### Difference from Yanyu's version\n",
    "\n",
    "Other than using a formal pipeline tool, differences are:\n",
    "\n",
    "1. Here samples are paired end reads -- this led to minor differences in `STAR` and `WASP` command parameters.\n",
    "2. No techinical replicates in this problem -- skipped Yanyu's step of merging technical replicates to one bam file.\n",
    "\n",
    "### Lessons learned: a retrospective view\n",
    "\n",
    "A list of cautions learned from running the pipeline:\n",
    "\n",
    "1. The `STAR` aligner is fast (compared to `tophat`) but requires over 30GB of memory. A desktop with 32GB of memory will not work for the `STAR` steps. I actually had to use a 64GB RAM desktop.\n",
    "2. `STAR`, when it fails (memory or not), will fail silently.\n",
    "3. Preparation of `STAR` database takes long time, but it is portable as long as one changes the absolute path in `genomeParameters.txt` to relative (ie, remove all the directories from file names only keep the basename). This is what I did. So the resource prepared by `[star]` workflow (below) can be copied to use in other machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and default workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "cwd = path('~/Documents/m6A/Data/ASE')\n",
    "parameter: ncpu = 20\n",
    "# Resource files\n",
    "resource_dir = f\"{cwd:a}/hg19\"\n",
    "ref_fa = \"hg19.fa\"\n",
    "ref_gtf = 'Homo_sapiens.GRCh38.91.gtf.gz'\n",
    "wasp_dir = f\"{cwd:a}/WASP-master/mapping\"\n",
    "samtools_snp_list = f\"{resource_dir}/020717_filtered_maf5_blacklist.final.bed\"\n",
    "# Sample files\n",
    "sample_dir = f\"{cwd:a}/samples\"\n",
    "from collections import OrderedDict\n",
    "## A list of sample names (keys) and their corresponding FASTQ files (values)\n",
    "samples = OrderedDict({'ENCLB279NMT': ['ENCFF824TZM', 'ENCFF176JNE']})\n",
    "fastq = paths([[f\"{sample_dir}/{s}/{q}.fastq.gz\" for q in samples[s]] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[default]\n",
    "sos_run('align+call')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource preparation\n",
    "\n",
    "### hg19 human reference data\n",
    "\n",
    "Obtain `hg19.fa` and `Homo_sapiens.GRCh38.91.gtf.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg19_reference_1 (download)]\n",
    "# Download `hg19.2bit` and `twoBitToFa` from {ucsc_url}\n",
    "ucsc_url = \"http://hgdownload.cse.ucsc.edu\"\n",
    "output: f\"{resource_dir}/hg19.2bit\", f\"{resource_dir}/twoBitToFa\"\n",
    "download: dest_dir = resource_dir, expand = True\n",
    "    {ucsc_url}/goldenPath/hg19/bigZips/hg19.2bit\n",
    "    {ucsc_url}/admin/exe/linux.x86_64/twoBitToFa\n",
    "\n",
    "[hg19_reference_2 (decompress hg19.fa)]\n",
    "# Use `twoBitToFa` to extract `hg19.fa` from `hg19.2bit`\n",
    "output: f\"{resource_dir}/{ref_fa}\"\n",
    "bash: expand = True\n",
    "    chmod +x {_input[1]}\n",
    "    {_input[1]} {_input[0]} {_output}\n",
    "\n",
    "[hg19_reference_3 (gene annotations)]\n",
    "# Download `Homo_sapiens.GRCh38.91.gtf.gz` from Ensembl\n",
    "# https://useast.ensembl.org/info/data/ftp/index.html\n",
    "ensembl_ftp = 'ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/'\n",
    "output: f\"{resource_dir}/{ref_gtf}\"\n",
    "download: dest_dir = resource_dir, expand = True\n",
    "    {ensembl_ftp}/{ref_gtf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[wasp]\n",
    "output: f\"{wasp_dir}/README.md\"\n",
    "download: decompress = True, dest_file = f'{cwd:a}/WASP.zip'\n",
    "    https://github.com/bmvdgeijn/WASP/archive/master.zip\n",
    "bash: expand = True\n",
    "    rm -f {cwd}/WASP.zip\n",
    "\n",
    "[star]\n",
    "# Quite time & resource consuming (3hrs, 32GB memory)\n",
    "depends: Py_Module('docker')\n",
    "output: f\"{resource_dir}/genomeParameters.txt\"\n",
    "bash: workdir = f'{cwd:a}', docker_image = 'bschiffthaler/ngs', expand=True\n",
    "     STAR --runMode genomeGenerate \\\n",
    "        --genomeDir {resource_dir} \\\n",
    "        --genomeFastaFiles {resource_dir}/{ref_fa} \\\n",
    "        --sjdbGTFtagExonParentTranscript {resource_dir}/{ref_gtf} \\\n",
    "        --runThreadN {ncpu}\n",
    "\n",
    "[quasar]\n",
    "depends: R_library(\"QuASAR@piquelab/QuASAR\")\n",
    "output: f'{resource_dir}/convertPileupToQuasar.R'\n",
    "download: dest_file = f'{resource_dir}/convertPileupToQuasar.R'\n",
    "    https://raw.githubusercontent.com/piquelab/QuASAR/master/scripts/convertPileupToQuasar.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also apparently `WASP` only works with Python 2 ... need to create a `conda` environment for it:\n",
    "\n",
    "```\n",
    "conda create -n py27 python=2.7\n",
    "source activate py27\n",
    "conda install pytables=2.4.0\n",
    "pip install pysam\n",
    "```\n",
    "\n",
    "and use \n",
    "\n",
    "```\n",
    "source activate py27\n",
    "```\n",
    "in `WASP` steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get samples\n",
    "\n",
    "FIXME: add description -- what are these samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[obtain_samples]\n",
    "# Download samples from ENCODE\n",
    "# https://www.encodeproject.org/experiments/ENCSR384KAN/\n",
    "encode_url = 'https://www.encodeproject.org/files'\n",
    "input: for_each = 'fastq', concurrent = True\n",
    "output: fastq, group_by = 1\n",
    "download: dest_dir = sample_dir, expand = True\n",
    "    {encode_url}/{_fastq:bnn}/@@download/{_fastq:b}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefiltering alignment\n",
    "\n",
    "Align with `STAR`, followd by `samtools` to remove reads with quality less than given cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[align_1 (STAR prefiltering alignment)]\n",
    "parameter: qual_cutoff = 10\n",
    "depends: sos_step('star')\n",
    "input: fastq, group_by = 2, concurrent = True\n",
    "output: [f\"{sample_dir}/{x}.qual{qual_cutoff}.bam\" for x in samples], group_by = 1\n",
    "bash: workdir = f'{cwd:a}', docker_image = 'bschiffthaler/ngs', expand=True\n",
    "    STAR --genomeDir {resource_dir} \\\n",
    "        --readFilesIn {_input[0]} {_input[1]} \\\n",
    "        --readFilesCommand zcat \\\n",
    "        --runThreadN {ncpu} --outStd BAM_SortedByCoordinate \\\n",
    "        --outSAMtype BAM SortedByCoordinate \\\n",
    "        --sjdbGTFtagExonParentTranscript {resource_dir}/{ref_gtf} |\n",
    "    samtools view -bq {qual_cutoff} > {_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WASP-informed remap\n",
    "\n",
    "Remap reads after [`WASP`](https://github.com/bmvdgeijn/WASP/tree/master/mapping) adjustment. `WASP` uses pre-defined list of SNPs and removes bias caused by them. The list was generated by Yanyu using 1k genome SNP in VCF format with MAF filter as input to `WASP/mapping/extract_vcf_snps.sh` command. Here I just take this pre-compiled list from Yanyu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[align_2 (WASP intersecting SNP): shared = {'wasp_split': 'step_output'}]\n",
    "# WASP finding unbiased reads intersecting with SNP\n",
    "depends: sos_step('wasp')\n",
    "input: group_by = 1, concurrent = True\n",
    "output: [[f\"{x:n}.remap.fq1.gz\", f\"{x:n}.remap.fq2.gz\", f\"{x:n}.to.remap.bam\", f\"{x:n}.keep.bam\"] for x in _input], group_by = 4\n",
    "bash: workdir = f'{cwd:a}', expand = True\n",
    "    source activate py27\n",
    "    python {wasp_dir}/find_intersecting_snps.py {_input} \\\n",
    "        --snp_dir {resource_dir}/wasp_snp_list \\\n",
    "        --is_sorted --is_paired_end\n",
    "\n",
    "[align_3 (STAR post alignment)]\n",
    "# Align WASP remap with STAR\n",
    "# Followd by samtools remove reads with quality less than {qual_cutoff}\n",
    "parameter: qual_cutoff = 10\n",
    "input: group_by = 4, pattern = '{name}.{qual}.remap.{ext}', concurrent = True\n",
    "output: expand_pattern(f'{_name[0]}.remapped.qual{qual_cutoff}.bam')\n",
    "bash: workdir = f'{cwd:a}', docker_image = 'bschiffthaler/ngs', expand=True\n",
    "    STAR --genomeDir {resource_dir} \\\n",
    "        --readFilesIn {_input[0]} {_input[1]} \\\n",
    "        --readFilesCommand zcat \\\n",
    "        --runThreadN {ncpu} --outStd BAM_SortedByCoordinate \\\n",
    "        --outSAMtype BAM SortedByCoordinate \\\n",
    "        --sjdbGTFtagExonParentTranscript {resource_dir}/{ref_gtf} |\n",
    "    samtools view -bq {qual_cutoff} > {_output}\n",
    "\n",
    "[align_4 (WASP remove ambiguously mapped reads)]\n",
    "to_remap = paths([wasp_split[i:i+4][2] for i in range(0, len(wasp_split), 4)])\n",
    "input: group_by = 1, paired_with = 'to_remap', pattern = '{name}.remapped.{ext}', concurrent = True\n",
    "output: expand_pattern(f'{_name[0]}.keep_remapped.{_ext[0]}')\n",
    "bash: workdir = f'{cwd:a}', expand=True, stderr = f'{_output:n}.log'\n",
    "    source activate py27\n",
    "    python {wasp_dir}/filter_remapped_reads.py {_to_remap} {_input} {_output}\n",
    "\n",
    "[align_5 (Merge WASP adjusted and originally kept BAM)]\n",
    "kept = paths([wasp_split[i:i+4][3] for i in range(0, len(wasp_split), 4)])\n",
    "input: group_by = 1, paired_with = 'kept', pattern = '{name}.keep_remapped.{ext}', concurrent = True\n",
    "output: expand_pattern(f'{_name[0]}.wasp_remapped.{_ext[0]}')\n",
    "bash: workdir = f'{cwd:a}', docker_image = 'bschiffthaler/ngs', expand=True\n",
    "    samtools merge - {_input} {_kept} | samtools sort -o {_output}\n",
    "    samtools index {_output}\n",
    "\n",
    "[align_6 (WASP remove duplicate reads )]\n",
    "input: group_by = 1, pattern = '{name}.bam', concurrent = True\n",
    "output: expand_pattern(f'{_name[0]}.dedup.bam')\n",
    "bash: workdir = f'{cwd:a}', expand=True, stderr = f'{_output:n}.log'\n",
    "    source activate py27\n",
    "    python {wasp_dir}/rmdup.py {_input} {_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genotype and ASE calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-`QuASAR` calling\n",
    "\n",
    "This step extracts count information from BAM file based on SNP list and prepare it into `QuASAR` input format. Here we need the list of SNPs used in BED format, and reference genome `fasta` file. Yanyu has previously prepared this BED file. Here I just grab and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[call_1 (samtools pileup)]\n",
    "# A standard samtools mpileup call\n",
    "input: group_by = 1, pattern = '{name}.bam', concurrent = True\n",
    "output: expand_pattern(f'{_name[0]}.pileup.gz')\n",
    "bash: workdir = f'{cwd:a}', docker_image = 'bschiffthaler/ngs', expand=True\n",
    "    samtools mpileup -f {resource_dir}/{ref_fa} -l {samtools_snp_list} {_input} | bgzip > {_output}\n",
    "\n",
    "[call_2 (awk & bedtools filter pileup)]\n",
    "# Remove bad pileup and intersect only with list of SNPs provided\n",
    "input: group_by = 1, pattern = '{name}.pileup.gz', concurrent = True\n",
    "output: expand_pattern(f'{_name[0]}.pileup.bed.gz')\n",
    "bash: workdir = f'{cwd:a}', docker_image = 'biocontainers/bedtools', expand = '${ }'\n",
    "    cat ${_input} | \\\n",
    "    awk -v OFS='\\\\t' '{ if ($4>0 && $5 !~ /\\\\+[0-9]+[ACGTNacgtn]+/ && $5 !~ /-[0-9]+[ACGTNacgtn]+/ && $5 !~ /[^\\\\^]\\\\*/) print $1,$2-1,$2,$3,$4,$5,$6}' | \\\n",
    "    sortBed -i stdin | intersectBed -a stdin -b ${samtools_snp_list} -wo | \\\n",
    "    cut -f 1-7,11-14 | gzip > ${_output}\n",
    "\n",
    "[call_3 (convert to quasar format)]\n",
    "depends: sos_step('quasar')\n",
    "input: group_by = 1, pattern = '{name}.pileup.bed.gz', concurrent = True\n",
    "output: expand_pattern(f'{_name[0]}.quasar.in.gz')\n",
    "bash: workdir = f'{cwd:a}', expand = True\n",
    "    R --slave --args {_input} < {resource_dir}/convertPileupToQuasar.R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFE771"
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
